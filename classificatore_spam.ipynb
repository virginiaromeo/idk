{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2455add1250f18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Esplorazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.102239600Z",
     "start_time": "2024-07-02T15:14:26.904684300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importiamo tutte le dipendenze che ci possono servire \n",
    "\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import loguniform\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scikitplot.metrics import plot_roc_curve as auc_roc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, \\\n",
    "f1_score, roc_auc_score, roc_curve, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dc391d9f08cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.174705Z",
     "start_time": "2024-07-02T15:14:27.105900600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Spam.xls')\n",
    "\n",
    "\n",
    "\n",
    "target = 'spam'\n",
    "labels = ['Ham','Spam']\n",
    "features = [i for i in df.columns.values if i not in [target]]\n",
    "\n",
    "original_df = df.copy(deep=True)\n",
    "display(df.head())\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c77368815223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.190977200Z",
     "start_time": "2024-07-02T15:14:27.147711900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking the dtypes of all the columns\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50858368712b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.272732900Z",
     "start_time": "2024-07-02T15:14:27.166728700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Checking number of unique rows in each feature\n",
    "\n",
    "df.nunique().sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f9f201b9fbf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.306923700Z",
     "start_time": "2024-07-02T15:14:27.197017500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Checking number of unique rows in each feature\n",
    "\n",
    "nu = df[features].nunique().sort_values()\n",
    "nf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n",
    "\n",
    "for i in range(df[features].shape[1]):\n",
    "    if nu.values[i]<=7:cf.append(nu.index[i])\n",
    "    else: nf.append(nu.index[i])\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b48f35d2fd3f7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.464948500Z",
     "start_time": "2024-07-02T15:14:27.243152Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking the stats of all the columns\n",
    "\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfab74bdb2b6f40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7dfbc40f38d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.508099600Z",
     "start_time": "2024-07-02T15:14:27.361354400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let us first analyze the distribution of the target variable\n",
    "\n",
    "MAP={}\n",
    "for e, i in enumerate(df[target].unique()):\n",
    "    MAP[i]=labels[e]\n",
    "#MAP={0:'Not-Survived',1:'Survived'}\n",
    "df1 = df.copy()\n",
    "df1[target]=df1[target].map(MAP)\n",
    "explode=np.zeros(len(labels)) #vettore di 0 di lunghezza pari alle etichette \n",
    "explode[-1]=0.1 # serve a fini grafici \n",
    "print('\\033[1mTarget Variable Distribution'.center(55))\n",
    "plt.pie(df1[target].value_counts(), labels=df1[target].value_counts().index, counterclock=False, shadow=True, \n",
    "        explode=explode, autopct='%1.1f%%', radius=1, startangle=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# il dict MAP prende i valori unici e li enumera cioè fornisce l'indice, e mappa ogni valore unico con la labels\n",
    "# effettua una copia del dataframe ed effettua il mapping in df1 sfruttando il dict \n",
    "\n",
    "# Nota bene la variabile target sembra sbilanciata quindi si potrebbe pensare di effettuare data augmentation (aumento dei dati)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d2538d19193f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.742496Z",
     "start_time": "2024-07-02T15:14:27.476659700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualizziamo la matrice sparsa \n",
    "\n",
    "# Visualising the Sparse Matrix\n",
    "\n",
    "plt.figure(figsize=[15,30])\n",
    "plt.title('')\n",
    "plt.spy(df[:100].values, precision = 0.1, markersize = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8b7ff40ac008b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad97d2b93c7ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:27.743499300Z",
     "start_time": "2024-07-02T15:14:27.612929600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Si rimuovono le righe duplicate \n",
    "\n",
    "counter = 0\n",
    "r,c = original_df.shape\n",
    "\n",
    "df1 = df.copy()\n",
    "df1.drop_duplicates(inplace=True)\n",
    "df1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "if df1.shape==(r,c):\n",
    "    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\n",
    "else:\n",
    "    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped ---> {r-df1.shape[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bb0ff18de2c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:28.109472900Z",
     "start_time": "2024-07-02T15:14:27.650182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# controlliamo se ci sono elementi vuoti \n",
    "\n",
    "\n",
    "nvc = pd.DataFrame(df1.isnull().sum().sort_values(), columns=['Total Null Values'])\n",
    "nvc['Percentage'] = round(nvc['Total Null Values']/df1.shape[0],3)*100\n",
    "print(nvc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f1e7e17c9d39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notiamo che ci sono molti outliers all'interno del dataset si può provare a trattare questi valori mancanti \n",
    "\n",
    "# DATA AUGMENTATION\n",
    "\n",
    "Avevamo detto pure che il nostro dataset era leggermente sbilanciato quindi si può pensare di applicare una tecnica come lo SMOTE\n",
    "\n",
    "Questa tecnica è utilizzata per affrontare il problema degli insiemi di dati sbilanciati, dove una classe (tipicamente la classe minoritaria) ha molti meno campioni rispetto ad altre classi. Questo sbilanciamento può causare problemi nei modelli di machine learning, che tendono a favorire la classe maggioritaria.\n",
    "\n",
    "E' una tecnica di data augmentation \n",
    "La sua azione consiste in 3 step:\n",
    "\n",
    "1. Identificazione della classe minoritaria \n",
    "2. Selezione dei vicini: Per ogni campione minoritario, SMOTE seleziona alcuni dei suoi k-nearest neighbors (vicini più prossimi). (il numero di vicini è scelto da noi)\n",
    "3. Genera nuovi campioni: Per generare un nuovo campione sintetico, SMOTE sceglie uno dei vicini selezionati e crea un punto lungo la linea che collega il campione minoritario originale e il vicino selezionato. Questo punto viene scelto in modo casuale tra il campione originale e il vicino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958058b8503cad9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:28.259488400Z",
     "start_time": "2024-07-02T15:14:28.059735400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5 = df1.copy()\n",
    "\n",
    "print('Original class distribution:')\n",
    "print(df5[target].value_counts())\n",
    "\n",
    "xf = df5.columns\n",
    "X = df5.drop([target],axis=1)\n",
    "Y = df5[target]\n",
    "\n",
    "smote = SMOTE()\n",
    "X, Y = smote.fit_resample(X, Y)\n",
    "\n",
    "df5 = pd.DataFrame(X, columns=xf)\n",
    "df5[target] = Y\n",
    "\n",
    "print('\\nClass distribution after applying SMOTE Technique:',)\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a399231c457b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:28.405849800Z",
     "start_time": "2024-07-02T15:14:28.161797500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calcoliamo il dataset finale dopo che abbiamo terminato il preprocessing \n",
    "\n",
    "\n",
    "plt.title('Final Dataset Samples')\n",
    "plt.pie([df.shape[0], original_df.shape[0]-df1.shape[0], df5.shape[0]-df1.shape[0]], radius = 1, shadow=True,\n",
    "        labels=['Retained','Dropped','Augmented'], counterclock=False, autopct='%1.1f%%', pctdistance=0.9, explode=[0,0,0])\n",
    "plt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78, shadow=True, colors=['powderblue'])\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0mThe final dataset after cleanup has {} samples & {} rows.'.format(df.shape[1], df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924d6df7d39c370",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Data Manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70f38df0337c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:28.407838800Z",
     "start_time": "2024-07-02T15:14:28.329461500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the data intro training & testing sets\n",
    "\n",
    "df = df5.copy()\n",
    "df.columns=[i.replace('[','_') for i in df.columns]\n",
    "\n",
    "X = df.drop([target],axis=1)\n",
    "Y = df[target]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad0dc906c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:28.608032500Z",
     "start_time": "2024-07-02T15:14:28.395669Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Andiamo a standardizzare el nostre features \n",
    "\n",
    "#Feature Scaling (Standardization)\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "print('\\033[1mStandardardization on Training set'.center(100))\n",
    "Train_X_std = std.fit_transform(Train_X)\n",
    "Train_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\n",
    "display(Train_X_std.describe())\n",
    "\n",
    "print('\\n','\\033[1mStandardardization on Testing set'.center(100))\n",
    "Test_X_std = std.transform(Test_X)\n",
    "Test_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\n",
    "display(Test_X_std.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed47421b5c73a2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708db123f444a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:14:29.109499900Z",
     "start_time": "2024-07-02T15:14:28.560294400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Checking the correlation\n",
    "\n",
    "features = df.columns\n",
    "plt.figure(figsize=[12,10])\n",
    "plt.title('Features Correlation-Plot')\n",
    "sns.heatmap(df[features].corr(), vmin=-1, vmax=1, center=0) #, \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b9b2ae42293b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Questa matrice di correlazione tra le variabile ci danno molte informazione sulle loro relazioni. In questo particolar caso sembra esserci una multicollinearità nel set di dati \n",
    "\n",
    "\n",
    "La multicollinearità è una situazione che si verifica nei dati quando due o più variabili indipendenti sono altamente correlate tra loro. In altre parole, una variabile indipendente può essere spiegata in modo lineare da un'altra variabile (o da una combinazione di altre variabili) nel dataset.\n",
    "\n",
    "La multicollinearità può crare problemi come:\n",
    "1. Instabilità dei coefficienti: I coefficienti delle variabili indipendenti nel modello di regressione diventano instabili e possono cambiare significativamente con piccole modifiche nei dati.\n",
    "2. Difficoltà di intepretazione: Diventa difficile determinare l'effetto individuale di ciascuna variabile indipendente sul variabile dipendente perché le variabili indipendenti sono fortemente correlate tra loro.\n",
    "3. Riduzione della precisione: La precisione delle stime dei coefficienti diminuisce, il che significa che le inferenze statistiche (come i test di significatività) possono essere inaffidabili.\n",
    "\n",
    "Come gestisce la multicollinearità: \n",
    "\n",
    "1. Rimozione delle Variabili Correlate:\n",
    "Eliminare una delle variabili altamente correlate può ridurre la multicollinearità.\n",
    "\n",
    "2. Combinazione delle Variabili:\n",
    "Creare una nuova variabile che combina le variabili correlate (ad esempio, utilizzando la somma o la media) può ridurre la multicollinearità.\n",
    "\n",
    "3. Uso di Tecniche di Regolarizzazione:\n",
    "Tecniche come la regressione Ridge o Lasso possono ridurre l'effetto della multicollinearità penalizzando i coefficienti delle variabili.\n",
    "\n",
    "4. Principal Component Analysis (PCA):\n",
    "PCA riduce la dimensionalità del dataset trasformando le variabili in nuove componenti principali non correlate.\n",
    "\n",
    "\n",
    "\n",
    "Strategia per eliminare la multicollinearità:\n",
    "\n",
    "1. Manual Method - Variance Inflation Factor (VIF) # il VIF fornisce un'indicazione di quanto la varianza di un coefficiente di regressione stimato è gonfiata a causa della collinearità tra le variabili indipendenti.\n",
    "2. Automatic Method - Recursive Feature Elimination (RFE) # Recursive Feature Elimination (RFE) è una tecnica automatica di selezione delle caratteristiche che aiuta a identificare e rimuovere le caratteristiche meno rilevanti nel dataset (di solito utilizza un algoritmo come svm, regressione lineare, reti neurali ecc)\n",
    "3. Decomposition Method - Principle Component Analysis (PCA) # PCA riduce la dimensionalità del dataset trasformando le variabili in nuove componenti principali non correlate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23443f178acfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5a. Manual Method - VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab4b672024603a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:15:29.177882500Z",
     "start_time": "2024-07-02T15:14:29.091120500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the VIFs to remove multicollinearity\n",
    "\n",
    "DROP=[]; scores1=[]; scores2=[]\n",
    "#scores.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std)))\n",
    "scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "\n",
    "for i in range(len(X.columns.values)-1):\n",
    "    vif = pd.DataFrame()\n",
    "    Xs = X.drop(DROP,axis=1)\n",
    "    #print(DROP)\n",
    "    vif['Features'] = Xs.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(Xs.values, i) for i in range(Xs.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    vif.reset_index(drop=True, inplace=True)\n",
    "    DROP.append(vif.Features[0])\n",
    "    if vif.VIF[0]>1:\n",
    "        scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "        scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "      \n",
    "    #print(scores)\n",
    "    \n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "#plt.ylim([0.7,0.85])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf162d67e320675",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5b. Automatic Method - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101aeda38c6894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:16:15.157050300Z",
     "start_time": "2024-07-02T15:15:29.180880900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applying Recurrsive Feature Elimination\n",
    "\n",
    "# Running RFE with the output number of the variable equal to 10\n",
    "LR = LogisticRegression()#.fit(Train_X_std, Train_Y)\n",
    "scores1=[]; scores2=[]; scores3=[]\n",
    "scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n",
    "scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n",
    "\n",
    "\n",
    "for i in range(len(X.columns.values)):\n",
    "    rfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-i)   \n",
    "    rfe = rfe.fit(Train_X_std, Train_Y)\n",
    "    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n",
    "    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n",
    "\n",
    "    \n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "\n",
    "#plt.ylim([0.80,0.84])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2eb383cbe70eeb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5c. PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecd24fd07a6cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:16:15.395311200Z",
     "start_time": "2024-07-02T15:16:15.160242400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(Train_X_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "x_values = range(1, pca.n_components_+1)\n",
    "ax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\n",
    "ax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\n",
    "plt.plot([0,pca.n_components_+1],[0.90,0.90],'g--')\n",
    "plt.plot([43,43],[0,1], 'g--')\n",
    "ax.set_title('Explained variance of components')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4b62586390d72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:28.636113400Z",
     "start_time": "2024-07-02T15:16:15.398725100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores1=[]; scores2=[]\n",
    "for i in range(len(X.columns.values)):\n",
    "    pca = PCA(n_components=Train_X_std.shape[1]-i)\n",
    "    Train_X_std_pca = pca.fit_transform(Train_X_std)\n",
    "    #print('The shape of final transformed training feature set:')\n",
    "    #print(Train_X_std_pca.shape)\n",
    "    Train_X_std_pca = pd.DataFrame(Train_X_std_pca)\n",
    "\n",
    "    Test_X_std_pca = pca.transform(Test_X_std)\n",
    "    #print('\\nThe shape of final transformed testing feature set:')\n",
    "    #print(Test_X_std_pca.shape)\n",
    "    Test_X_std_pca = pd.DataFrame(Test_X_std_pca)\n",
    "    \n",
    "    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n",
    "    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n",
    "\n",
    "    \n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "#plt.ylim([0.80,0.84])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6cf6ad11245f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Inferenza: Nelle tecniche VIF, RFE e PCA, abbiamo notato punteggi migliori eliminando alcune caratteristiche multicollinari. Ma per evitare la maledizione della dimensionalità, possiamo catturare il 90% della varianza dei dati spiegata dalle prime n componenti PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38c2e377b08642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:28.688699800Z",
     "start_time": "2024-07-02T15:17:28.640509300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Applying PCA Transformations\n",
    "\n",
    "pca = PCA(n_components=32)\n",
    "\n",
    "Train_X_std_pca = pca.fit_transform(Train_X_std)\n",
    "print('The shape of final transformed training feature set:')\n",
    "print(Train_X_std_pca.shape)\n",
    "Train_X_std_pca = pd.DataFrame(Train_X_std_pca)\n",
    "\n",
    "Test_X_std_pca = pca.transform(Test_X_std)\n",
    "print('\\nThe shape of final transformed testing feature set:')\n",
    "print(Test_X_std_pca.shape)\n",
    "Test_X_std_pca = pd.DataFrame(Test_X_std_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f690b7042c6c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957ea87ffbf3d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:28.713507Z",
     "start_time": "2024-07-02T15:17:28.654362200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Let us create first create a table to store the results of various models \n",
    "\n",
    "Evaluation_Results = pd.DataFrame(np.zeros((8,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\n",
    "Evaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Naïve Bayes Classifier (NB)',\n",
    "                         'Support Vector Machine (SVM)','K Nearest Neighbours (KNN)', 'Gradient Boosting (GB)','Extreme Gradient Boosting (XGB)']\n",
    "Evaluation_Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902265b8fd466d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:28.717504600Z",
     "start_time": "2024-07-02T15:17:28.686730900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let us define functions to summarise the Prediction's scores .\n",
    "\n",
    "#Classification Summary Function\n",
    "def Classification_Summary(pred,pred_prob,i):\n",
    "    Evaluation_Results.iloc[i]['Accuracy']=round(accuracy_score(Test_Y, pred),3)*100   \n",
    "    Evaluation_Results.iloc[i]['Precision']=round(precision_score(Test_Y, pred, average='weighted'),3)*100 #\n",
    "    Evaluation_Results.iloc[i]['Recall']=round(recall_score(Test_Y, pred, average='weighted'),3)*100 #\n",
    "    Evaluation_Results.iloc[i]['F1-score']=round(f1_score(Test_Y, pred, average='weighted'),3)*100 #\n",
    "    Evaluation_Results.iloc[i]['AUC-ROC score']=round(roc_auc_score(Test_Y, pred_prob[:,1], multi_class='ovr'),3)*100 #[:, 1]\n",
    "    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*35,Evaluation_Results.index[i], '-'*35,'>'*3))\n",
    "    print('Accuracy = {}%'.format(round(accuracy_score(Test_Y, pred),3)*100))\n",
    "    print('F1 Score = {}%'.format(round(f1_score(Test_Y, pred, average='weighted'),3)*100)) #\n",
    "    print('\\n \\033[1mConfusiton Matrix:\\033[0m\\n',confusion_matrix(Test_Y, pred))\n",
    "    print('\\n\\033[1mClassification Report:\\033[0m\\n',classification_report(Test_Y, pred))\n",
    "    \n",
    "    auc_roc(Test_Y, pred_prob, curves=['each_class'])\n",
    "    plt.show()\n",
    "\n",
    "#Visualising Function\n",
    "def AUC_ROC_plot(Test_Y, pred):    \n",
    "    ref = [0 for _ in range(len(Test_Y))]\n",
    "    ref_auc = roc_auc_score(Test_Y, ref)\n",
    "    lr_auc = roc_auc_score(Test_Y, pred)\n",
    "\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(Test_Y, ref)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(Test_Y, pred)\n",
    "\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='AUC = {}'.format(round(roc_auc_score(Test_Y, pred)*100,2))) \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a216709feae30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6a. Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0529eb71b39ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:31.218730700Z",
     "start_time": "2024-07-02T15:17:28.701349400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression Classifier\n",
    "\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'liblinear'] # 'lbfgs',\n",
    "space['penalty'] = ['l2'] #'none','l1','elasticnet'\n",
    "space['C'] = loguniform(1e-5, 100)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(LR_model, space, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "LR = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = LR.predict(Test_X_std)\n",
    "pred_prob = LR.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,0)\n",
    "\n",
    "print('\\n\\033[1mInterpreting the Output of Logistic Regression:\\n\\033[0m')\n",
    "\n",
    "print('intercept ', LR.intercept_[0])\n",
    "print('classes', LR.classes_)\n",
    "display(pd.DataFrame({'coeff': LR.coef_[0]}, index=Train_X_std.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b87950f69519a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6b Decision Tree Classificatore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacc2c4078aadd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:39.029185200Z",
     "start_time": "2024-07-02T15:17:31.222256200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building Decision Tree Classifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(DT_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "DT = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = DT.predict(Test_X_std)\n",
    "pred_prob = DT.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,1)\n",
    "\n",
    "print('\\n\\033[1mInterpreting the output of Decision Tree:\\n\\033[0m')\n",
    "tree.plot_tree(DT)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb92c28e308b0f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6c Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35505020556ae46a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:49.422778900Z",
     "start_time": "2024-07-02T15:17:39.033351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Random-Forest Classifier\n",
    "\n",
    "RF_model = RandomForestClassifier()\n",
    "\n",
    "param_dist={'bootstrap': [True, False],\n",
    "            'max_depth': [10, 20, 50, 100, None],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [50, 100]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(RF_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "RF = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = RF.predict(Test_X_std)\n",
    "pred_prob = RF.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,2)\n",
    "\n",
    "print('\\n\\033[1mInterpreting the output of Random Forest:\\n\\033[0m')\n",
    "rfi=pd.Series(RF.feature_importances_, index=Train_X_std.columns).sort_values(ascending=False)\n",
    "plt.barh(rfi.index,rfi.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611111f5b61d29d4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6d Naive Bayes Classfier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e23eec8729ad12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:17:49.737317900Z",
     "start_time": "2024-07-02T15:17:49.425815900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Naive Bayes Classifier\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, params, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "NB = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = NB.predict(Test_X_std)\n",
    "pred_prob = NB.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e1089acd96090",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6e SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167ffd249576897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:20:07.439472800Z",
     "start_time": "2024-07-02T15:17:49.739283500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Support Vector Machine Classifier\n",
    "\n",
    "SVM_model = SVC(probability=True).fit(Train_X_std, Train_Y)\n",
    "\n",
    "svm_param = {\"C\": [.01, .1, 1, 5, 10, 100],             \n",
    "             \"gamma\": [.01, .1, 1, 5, 10, 100],\n",
    "             \"kernel\": [\"rbf\"],\n",
    "             \"random_state\": [1]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(SVM_model, svm_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "SVM = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = SVM.predict(Test_X_std)\n",
    "pred_prob = SVM.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391509e996dc3d2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6f. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce4eb887efef58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:21:08.787707Z",
     "start_time": "2024-07-02T15:20:58.364452400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building K-Neareset Neighbours Classifier\n",
    "\n",
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "knn_param = {\"n_neighbors\": [i for i in range(1,30,5)],\n",
    "             \"weights\": [\"uniform\", \"distance\"],\n",
    "             \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "             \"leaf_size\": [1, 10, 30],\n",
    "             \"p\": [1,2]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, knn_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = KNN.predict(Test_X_std)\n",
    "pred_prob = KNN.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80c09b49f01206",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6g. Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e980c0d19ff69c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:21:37.518268400Z",
     "start_time": "2024-07-02T15:21:33.949389200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Gradient Boosting Classifier\n",
    "\n",
    "GB_model = GradientBoostingClassifier().fit(Train_X_std, Train_Y)\n",
    "param_dist = {\n",
    "    \"n_estimators\":[5,20,100,500],\n",
    "    \"max_depth\":[1,3,5,7,9],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#RCV = RandomizedSearchCV(GB_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n",
    "\n",
    "GB = GB_model.fit(Train_X_std, Train_Y)#.best_estimator_\n",
    "pred = GB.predict(Test_X_std)\n",
    "pred_prob = GB.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882e6dc8beb5c15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sommario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eb41dfe3509f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:24:10.459990500Z",
     "start_time": "2024-07-02T15:24:09.128136900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Plotting Confusion-Matrix of all the predictive Models\n",
    "\n",
    "def plot_cm(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.columns=labels\n",
    "    cm.index=labels\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    #fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=annot, fmt='')# cmap= \"GnBu\"\n",
    "    \n",
    "def conf_mat_plot(all_models):\n",
    "    plt.figure(figsize=[20,3.5*math.ceil(len(all_models)*len(labels)/14)])\n",
    "    \n",
    "    for i in range(len(all_models)):\n",
    "        if len(labels)<=4:\n",
    "            plt.subplot(2,4,i+1)\n",
    "        else:\n",
    "            plt.subplot(math.ceil(len(all_models)/3),3,i+1)\n",
    "        pred = all_models[i].predict(Test_X_std)\n",
    "        #plot_cm(Test_Y, pred)\n",
    "        sns.heatmap(confusion_matrix(Test_Y, pred), annot=True, cmap='Blues', fmt='.0f') #vmin=0,vmax=5\n",
    "        plt.title(Evaluation_Results.index[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "conf_mat_plot([LR,DT,RF,NB,SVM,KNN,GB])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca686feb97cfdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:24:27.828288400Z",
     "start_time": "2024-07-02T15:24:27.536703100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comparing all the models Scores\n",
    "\n",
    "print('\\033[1mML Algorithms Comparison'.center(100))\n",
    "plt.figure(figsize=[12,8])\n",
    "sns.heatmap(Evaluation_Results, annot=True, vmin=90, vmax=100, cmap='Blues', fmt='.1f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34046944d4ef10d3",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10f425d965d49384",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Approfondimenti: \n",
    "Per il problema attuale, è più importante concentrarsi sul punteggio di precisione. Dalla mappa di calore sopra riportata possiamo notare che i modelli Random Forest e Boosting si sono comportati bene sul dataset attuale..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea14678b7411aa5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  Risultati e conclusioni del progetto\n",
    "\n",
    "Ecco alcuni dei principali risultati del progetto:\n",
    "\n",
    "1. Il set di dati era piuttosto piccolo, per un totale di circa 4600 campioni e dopo la pre-elaborazione è stato eliminato il 14,6% dei campioni.\n",
    "2. I campioni erano leggermente sbilanciati dopo l'elaborazione, quindi la tecnica SMOTE è stata applicata ai dati per bilanciare le classi, aggiungendo il 16,7% di campioni in più al dataset.\n",
    "3. La visualizzazione della distribuzione dei dati e delle loro relazioni ci ha aiutato a capire la relazione tra i set di caratteristiche.\n",
    "4. È stata effettuata la selezione/eliminazione delle caratteristiche e sono state selezionate quelle appropriate.\n",
    "5. Il test di più algoritmi con iperparametri di precisione ci ha permesso di capire le prestazioni dei modelli di vari algoritmi su questo specifico set di dati.\n",
    "6. Il classificatore Random Forest e XG-Boost hanno ottenuto risultati eccezionali sul dataset attuale, considerando il punteggio di precisione come parametro chiave.\n",
    "7. Tuttavia, è saggio prendere in considerazione anche modelli più semplici come la Regressione logistica, poiché è più generalizzabile e meno costosa dal punto di vista computazionale, ma comporta il costo di lievi errori di classificazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd832e512424c7f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
